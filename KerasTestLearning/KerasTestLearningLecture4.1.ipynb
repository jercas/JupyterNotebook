{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Keras中数据处理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 文字预处理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1.1 字符拆分 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['those', 'that', 'come', 'through', 'the', 'ivory']\n",
      "['Those', 'that', 'come', 'through', 'the', 'Ivory']\n",
      "['ose', 't', 't', 'come', 't', 'roug']\n"
     ]
    }
   ],
   "source": [
    "# 导入keras文字预处理函数\n",
    "import keras.preprocessing.text as tp\n",
    "import numpy as np\n",
    "\n",
    "txt1 = \"Those that come through the Ivory Gate cheat us with empty promises that never see fulfillment.\" \n",
    "txt2 = \" Those that come through the Gate of Horn inform the dreamer of truth.\"\n",
    "txt = txt1 + txt2\n",
    "\n",
    "# 字符拆分\n",
    "out1 = tp.text_to_word_sequence(txt)\n",
    "print(out1[:6])\n",
    "\n",
    "# 允许大小写\n",
    "out2 = tp.text_to_word_sequence(txt,lower=False)\n",
    "print(out2[:6])\n",
    "\n",
    "# 设置 分割符为 T h a\n",
    "out3 = tp.text_to_word_sequence(txt,lower=False, filters=\"Tha\")\n",
    "print(out3[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 ['此', '开卷', '第一回', '也', '。', '作者', '自云', '：']\n",
      "324 ['此', '开卷', '第一', '第一回', '一回', '也', '', '', '作者', '自', '云', '', '', '曾', '历']\n",
      "324 ['此', '开卷', '第一', '第一回', '一回', '也', '', '', '作者', '自', '云', '', '', '曾', '历']\n"
     ]
    }
   ],
   "source": [
    "# 导入中文分词包 jieba分词\n",
    "import jieba\n",
    "chn = \"此开卷第一回也。作者自云：曾历过一番梦幻之后，故将真事隐去，而借通灵说此《石头记》一书也，故曰“甄士隐”云云。但书中所记何事何人?自己又云：“今风尘碌碌，一事无成，忽念及当日所有之女子：一一细考较去，觉其行止见识皆出我之上。我堂堂须眉诚不若彼裙钗，我实愧则有馀，悔又无益，大无可如何之日也。当此日，欲将已往所赖天恩祖德，锦衣纨之时，饫甘餍肥之日，背父兄教育之恩，负师友规训之德，以致今日一技无成、半生潦倒之罪，编述一集，以告天下；知我之负罪固多，然闺阁中历历有人，万不可因我之不肖，自护己短，一并使其泯灭也。所以蓬牖茅椽，绳床瓦灶，并不足妨我襟怀；况那晨风夕月，阶柳庭花，更觉得润人笔墨。我虽不学无文，又何妨用假语村言敷演出来?亦可使闺阁昭传。复可破一时之闷，醒同人之目，不亦宜乎？”故曰“贾雨村”云云。\"\n",
    "\n",
    "out4 = jieba.lcut(chn, cut_all=False)\n",
    "print (len(out4), out4[:8])\n",
    "# 细颗粒分割\n",
    "out5 = jieba.lcut(chn, cut_all=True)\n",
    "print (len(out5), out5[:15])\n",
    "# 使用HMM模型\n",
    "out6 = jieba.lcut(chn, cut_all=True, HMM=True)\n",
    "print (len(out6), out6[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1.2 建立索引 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cheat': 28,\n",
       " 'come': 27,\n",
       " 'dreamer': 25,\n",
       " 'empty': 24,\n",
       " 'fulfillment': 23,\n",
       " 'gate': 22,\n",
       " 'horn': 20,\n",
       " 'inform': 19,\n",
       " 'ivory': 18,\n",
       " 'never': 17,\n",
       " 'of': 16,\n",
       " 'promises': 14,\n",
       " 'see': 13,\n",
       " 'that': 12,\n",
       " 'the': 9,\n",
       " 'those': 6,\n",
       " 'through': 4,\n",
       " 'truth': 2,\n",
       " 'us': 1,\n",
       " 'with': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过 text_to_word_sequence 生成的变量，直接可按以下方式生成索引\n",
    "# 对分割字符串进行反向排序\n",
    "out1.sort(reverse = True)\n",
    "\n",
    "# 1. zip命令将每个单词依次与序号配对，其中np.arange生成一个out1长度大小的numpy数组\n",
    "# 2. list命令将配对的数据改为列表\n",
    "# 3. dict命令将列表修改为字典\n",
    "dict(list(zip(out1, np.arange(len(out1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "1 2 3\n",
      "2 2 3\n",
      "3 1 2\n",
      "4 1 2\n",
      "5 2 3\n",
      "6 1 2\n",
      "7 0 1\n",
      "8 0 1\n",
      "9 0 1\n",
      "10 1 2\n",
      "11 2 3\n",
      "12 2 3\n",
      "13 0 1\n"
     ]
    }
   ],
   "source": [
    "# 试用OneHot编码法，建立索引\n",
    "# 示例\n",
    "xin = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "tout =(tp.text_to_word_sequence(str(xin)))\n",
    "\n",
    "# one_hot(x,y)  --  x:待索引的字符串列表,y:最大索引值n\n",
    "xout = tp.one_hot(str(xin),5)\n",
    "for s in range(len(xin)):\n",
    "    print(s, hash(tout[s]) % (5-1), xout[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1.3 序列补齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [0 0 4 5]\n",
      " [6 7 8 9]]\n",
      "====\n",
      "[[ 1  2  3 88 88]\n",
      " [ 4  5 88 88 88]\n",
      " [ 6  7  8  9 88]]\n",
      "====\n",
      "[[1 2 3]\n",
      " [4 5 0]\n",
      " [7 8 9]]\n",
      "====\n",
      "[[1 2 3]\n",
      " [0 4 5]\n",
      " [7 8 9]]\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x = [[1,2,3],[4,5],[6,7,8,9]]\n",
    "y0 = pad_sequences(x)\n",
    "\n",
    "# maxlen 指定补齐序列的长度，默认按照最长的列表长度来设定（如y0）；\n",
    "#                            若设定长度超出列表元素长度则以value属性指定的填充符来扩充（如y1，以88作为填充符来补长）；\n",
    "#                            若设定长度小于列表元素长度则产生截断（如y2、y3），截断标准为如果补齐序列长度为k，则保留列表元素最后k个索引，如y3；\n",
    "#\n",
    "# padding 指定是从后面(post，如y1、y2)还是前面补齐(pre，如y3；默认从前面补齐,如y0)，补齐索引数字默认为 0 -- 可通过value修改；\n",
    "y1 = pad_sequences(x, maxlen=5, padding='post', value=88)\n",
    "y2 = pad_sequences(x, maxlen=3, padding='post')\n",
    "y3 = pad_sequences(x, maxlen=3 ,padding='pre')\n",
    "\n",
    "print(\"{}\\n====\".format(y0))\n",
    "print(\"{}\\n====\".format(y1))\n",
    "print(\"{}\\n====\".format(y2))\n",
    "print(\"{}\\n====\".format(y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1.4 转换矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b3d64ff98f9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_sentence_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b3d64ff98f9c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_sentence_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用pad_sequences转换矩阵\n",
    "max_sentence_len = 50\n",
    "x =[]\n",
    "for sentences in txt:\n",
    "    x = [word_idx[w] for w in sentences]\n",
    "    x.append(x)\n",
    "pad_sequences(x, maxlen = max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
